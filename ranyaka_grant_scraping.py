# -*- coding: utf-8 -*-
"""Ranyaka Grant Scraping

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XJlNsh00G9ZDqHRpExfmdElzrhS3wwKi

#Ranyaka Grant Scraping

The purpose of this short piece of code is to provide a way for Ranyaka to scrape potential grant-granting candidates that specifically target African, South African, and community-wellness oriented NGOs.

Below are some preliminary instructions on how this code will be structured:

1. Establish connection to the grant-hosting web client using XML. http://www.fundsnetservices.com/searchresult/30/International-Grants-&-Funders/1.html

2. Extract the entire dataframe based on all 180+ entries under this URL.

3. Provide a user-friendly editable piece of code to sort through the rows conveniently as well as a way to download in Excel format.
"""

!git init
!git config — global user.email “anderszhou1000@gmail.com”
!git config — global user.name “andersthequeen”

! ls

"""#XPath"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip3 install requests_html
# 
# import urllib
# import requests
# import pandas as pd
# from lxml import html
# from lxml import etree
# from io import StringIO, BytesIO
# from requests_html import HTML, HTMLSession
# from bs4 import BeautifulSoup

# NOTE (1): HOW CAN WE BUILD ON THIS FUNCTIONALITY?
scrape_df = pd.DataFrame(columns = ['Grant', 'Text', 'URL'])
url_list = []
for i in range(0,19):
  url_list.append("http://www.fundsnetservices.com/searchresult/30/International-Grants-&-Funders/" 
                  + str(i+1) + ".html")

session = HTMLSession()
for url in url_list:
  r = session.get(url)
  soup = BeautifulSoup(r.text, 'html.parser')
  ps = soup.findAll("p", attrs = {"class": "tdclass"})
  for p in ps:
    a = str(p.find('a').contents[0]).strip()
    b = str(p.contents[4]).strip()
    c = p.find('a').get('href')
    scrape_df = scrape_df.append({'Grant' : a, 'Text' : b, 'URL' : c}, 
                   ignore_index = True)
scrape_df

# NOTE (2): CONVERT TO EXCEL FILE
scrape_df.to_excel('grant_scrape.xlsx')

from google.colab import files
files.download('grant_scrape.xlsx')